name: PPO

config:
  num_workers: 8
  num_envs_per_worker: 1

  gamma: 0.999 # 0.98 0.99 0.995 0.999

  rollout_fragment_length: 500
  train_batch_size: 4000
  num_sgd_iter: 2
  lr: 1e-4
  sgd_minibatch_size: 1000
  batch_mode: "truncate_episodes"

  entropy_coeff: 1e-4 # 1e-3, 1e-7
  lr_schedule: [
    [       0, 2e-4], # 1e-3, 1e-5
    [  250000, 1e-5],
    [ 2000000, 1e-6],
  ]